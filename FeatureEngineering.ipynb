{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FeatureEngineering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6RNqe4_rBlv"
      },
      "source": [
        "<h1><strong><font color='green'>Feature Engineering</font></strong></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CzLptGNJEnl"
      },
      "source": [
        "As part of Feature engineering we will extract these features from the text data :\r\n",
        "<li>Creating tfidf-w2v representation using pretrained glove model (300-dim)</li>\r\n",
        "<li>Length of the text by word level and character level (2-dim)</li>\r\n",
        "<li>Sentiment scores of text data using nltk (4-dim)</li>\r\n",
        "\r\n",
        "We will get a total of 306 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpR9YAzGfod7"
      },
      "source": [
        "#Importing the packages\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "import re\r\n",
        "import pickle\r\n",
        "from wordcloud import WordCloud,STOPWORDS\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.snowball import SnowballStemmer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import nltk\r\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\r\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pun5F_8Ise7B"
      },
      "source": [
        "#Creating input and target data\r\n",
        "\r\n",
        "X_train,y_train=pd.DataFrame(final_train.iloc[:,0]),final_train.iloc[:,1:]\r\n",
        "X_cv,y_cv=pd.DataFrame(final_cv.iloc[:,0]),final_cv.iloc[:,1:]\r\n",
        "X_test,y_test=pd.DataFrame(final_test.iloc[:,0]),final_test.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekldg-P0tlR4",
        "outputId": "5449dab7-1253-4d1b-a8d0-85182f2c21af"
      },
      "source": [
        "print('Shapes of train,cross validate and test data :')\r\n",
        "print('X_ train shape : ',X_train.shape,',  y_train shape : ', y_train.shape)\r\n",
        "print('X_ cv shape :     ',X_cv.shape, '   ,  y_cv shape : ',y_cv.shape)\r\n",
        "print('X_ test shape :  ',X_test.shape, ',  y_test shape : ',y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapes of train,cross validate and test data :\n",
            "X_ train shape :  (6659, 1) ,  y_train shape :  (6659, 3)\n",
            "X_ cv shape :      (940, 1)    ,  y_cv shape :  (940, 3)\n",
            "X_ test shape :   (1608, 1) ,  y_test shape :  (1608, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RJnTlQlSUAj5"
      },
      "source": [
        "fasttext_word_embeddings = dict()\r\n",
        "fasttext_model = open('wiki-news-300d-1M-subword.vec',encoding='utf-8')\r\n",
        "for line in fasttext_model: # For each line in vector file\r\n",
        "  row = line.split() \r\n",
        "  word = row[0] # First word \r\n",
        "  vector = np.asarray(row[1:], dtype='float32') # Remaining text is 300 dimensional embeddings\r\n",
        "  fasttext_word_embeddings[word] = vector # Creating a dictionay with word as key and embeddings  vector as value\r\n",
        "fasttext_model.close()\r\n",
        "\r\n",
        "fasttext_words=fasttext_word_embeddings.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTt4CyAR23-m"
      },
      "source": [
        "words_to_emebd=set()\r\n",
        "for index,row in X_train.iterrows():\r\n",
        "  text=row['Description'].split()\r\n",
        "  for word in text:\r\n",
        "    words_to_emebd.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IBtilVa4z4c"
      },
      "source": [
        "for index,row in X_cv.iterrows():\r\n",
        "  text=row['Description'].split()\r\n",
        "  for word in text:\r\n",
        "    words_to_emebd.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5urMMHW3XTZ"
      },
      "source": [
        "for index,row in X_test.iterrows():\r\n",
        "  text=row['Description'].split()\r\n",
        "  for word in text:\r\n",
        "    words_to_emebd.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0bgyYneI7XE"
      },
      "source": [
        "#Getting the word embedding from pretrained glove model.\r\n",
        "\r\n",
        "glove_word_embeddings = dict()\r\n",
        "glove_model = open('glove.6B.300d.txt')\r\n",
        "for line in glove_model: # For each line in text file\r\n",
        "  row = line.split() \r\n",
        "  word = row[0] # First word \r\n",
        "  if word in words_to_embed:\r\n",
        "    vector = np.asarray(row[1:], dtype='float32') # Remaining text is 300 dimensional embeddings\r\n",
        "    glove_word_embeddings[word] = vector # Creating a dictionay with word as key and embeddings  vector as value\r\n",
        "glove_model.close()\r\n",
        "\r\n",
        "glove_words=glove_word_embeddings.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiIjrtL-hD2i"
      },
      "source": [
        "pickle.dump(glove_word_embeddings,open('small_glove_embeddings.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kAiPa9jWTEi"
      },
      "source": [
        "def get_embedding_features(data,word_embeddings,model_words):\r\n",
        "  '''This function takes dataframe as input and returns fasttext vecotr respresent of text data(Description)'''\r\n",
        "  vector_rep=[]\r\n",
        "  preprocessed_descriptions = data['Description'].values\r\n",
        "  for text in preprocessed_descriptions: # For each description\r\n",
        "    vector=np.zeros(300)\r\n",
        "    n=0\r\n",
        "    for word in text.split():# For each word in vector\r\n",
        "      if (word in model_words):\r\n",
        "        vec=word_embeddings[word] #Getting the word's w2v representation\r\n",
        "        vector+=vec\r\n",
        "        n+=1\r\n",
        "    if n!=0:\r\n",
        "      vector/=n\r\n",
        "    vector_rep.append(vector)\r\n",
        "  return np.array(vector_rep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOK4vRuQIOks"
      },
      "source": [
        "tfidf_model = TfidfVectorizer() \r\n",
        "tfidf_model.fit(X_train['Description'].values) # Fitting the tfidf vectorizer on train data\r\n",
        "# we are converting a dictionary with word as a key, and the idf as a value\r\n",
        "idf_dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\r\n",
        "tfidf_words = set(tfidf_model.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeAGqcpvIQej"
      },
      "source": [
        "def tfidf_w2v(data,word_embeddings,model_words):\r\n",
        "  '''This function takes dataframe as input and returns tfidf-w2v vecotr respresent of text data(Description)'''\r\n",
        "  tfidf_w2v_vectors=[]\r\n",
        "  preprocessed_descriptions = data['Description'].values\r\n",
        "  for text in preprocessed_descriptions: # For each description\r\n",
        "    vector=np.zeros(300) #Intialising a 300-dim vector\r\n",
        "    tf_idf_weight =0\r\n",
        "    for word in text.split(): # For each word in vector\r\n",
        "      if (word in model_words) and (word in tfidf_words):\r\n",
        "        vec=word_embeddings[word] #Getting the word's w2v representation\r\n",
        "        tf_idf = idf_dictionary[word]*(text.count(word)/len(text.split())) # Calulating tfidf value of word using idf values\r\n",
        "        vector += (vec * tf_idf) # Computing weighted sum of tfidf-w2v\r\n",
        "        tf_idf_weight += tf_idf\r\n",
        "    if tf_idf_weight!=0:\r\n",
        "      vector/=tf_idf_weight # Averaging the weighted sum of tfidf-w2v\r\n",
        "    tfidf_w2v_vectors.append(vector)\r\n",
        "  return np.array(tfidf_w2v_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa6j0EATXz9K",
        "outputId": "0ca81399-8ca4-400e-a747-3478a2ff2c7a"
      },
      "source": [
        "X_train_fasttext_w2v=get_embedding_features(X_train,fasttext_word_embeddings,fasttext_words)\r\n",
        "X_cv_fasttext_w2v=get_embedding_features(X_cv,fasttext_word_embeddings,fasttext_words)\r\n",
        "X_test_fasttext_w2v=get_embedding_features(X_test,fasttext_word_embeddings,fasttext_words)\r\n",
        "X_train_fasttext_w2v=pd.DataFrame(X_train_fasttext_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "X_cv_fasttext_w2v=pd.DataFrame(X_cv_fasttext_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "X_test_fasttext_w2v=pd.DataFrame(X_test_fasttext_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "print('\\nAfter fasttext-w2v\\n')\r\n",
        "print('Shapes of tfidf-w2v features on train data : ',X_train_fasttext_w2v.shape)\r\n",
        "print('Shapes of tfidf-w2v features on cv data : ',X_cv_fasttext_w2v.shape)\r\n",
        "print('Shapes of tfidf-w2v features on test data : ',X_test_fasttext_w2v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "After fasttext-w2v\n",
            "\n",
            "Shapes of tfidf-w2v features on train data :  (6659, 300)\n",
            "Shapes of tfidf-w2v features on cv data :  (940, 300)\n",
            "Shapes of tfidf-w2v features on test data :  (1608, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLdo94g2JToe",
        "outputId": "179b4d70-7181-4b57-82ef-d0e3ade294fd"
      },
      "source": [
        "X_train_fasttext_tfidf_w2v=tfidf_w2v(X_train,fasttext_word_embeddings,fasttext_words)\r\n",
        "X_cv_fasttext_tfidf_w2v=tfidf_w2v(X_cv,fasttext_word_embeddings,fasttext_words)\r\n",
        "X_test_fasttext_tfidf_w2v=tfidf_w2v(X_test,fasttext_word_embeddings,fasttext_words)\r\n",
        "X_train_fasttext_tfidf_w2v=pd.DataFrame(X_train_fasttext_tfidf_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "X_cv_fasttext_tfidf_w2v=pd.DataFrame(X_cv_fasttext_tfidf_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "X_test_fasttext_tfidf_w2v=pd.DataFrame(X_test_fasttext_tfidf_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "print('\\nAfter fasttext-tfidf \\n')\r\n",
        "print('Shapes of tfidf-w2v features on train data : ',X_train_fasttext_tfidf_w2v.shape)\r\n",
        "print('Shapes of tfidf-w2v features on cv data : ',X_cv_fasttext_tfidf_w2v.shape)\r\n",
        "print('Shapes of tfidf-w2v features on test data : ',X_test_fasttext_tfidf_w2v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "After fasttext-tfidf \n",
            "\n",
            "Shapes of tfidf-w2v features on train data :  (6659, 300)\n",
            "Shapes of tfidf-w2v features on cv data :  (940, 300)\n",
            "Shapes of tfidf-w2v features on test data :  (1608, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awEQpcQbITlE",
        "outputId": "5c125a60-3543-4ca4-f47e-339e9cb20e75"
      },
      "source": [
        "X_train_glove_w2v=get_embedding_features(X_train,glove_word_embeddings,glove_words)\r\n",
        "X_cv_glove_w2v=get_embedding_features(X_cv,glove_word_embeddings,glove_words)\r\n",
        "X_test_glove_w2v=get_embedding_features(X_test,glove_word_embeddings,glove_words)\r\n",
        "X_train_glove_w2v=pd.DataFrame(X_train_glove_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "X_cv_glove_w2v=pd.DataFrame(X_cv_glove_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "X_test_glove_w2v=pd.DataFrame(X_test_glove_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "print('\\nAfter glove-w2v\\n')\r\n",
        "print('Shapes of tfidf-w2v features on train data : ',X_train_glove_w2v.shape)\r\n",
        "print('Shapes of tfidf-w2v features on cv data : ',X_cv_glove_w2v.shape)\r\n",
        "print('Shapes of tfidf-w2v features on test data : ',X_test_glove_w2v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "After glove-w2v\n",
            "\n",
            "Shapes of tfidf-w2v features on train data :  (6659, 300)\n",
            "Shapes of tfidf-w2v features on cv data :  (940, 300)\n",
            "Shapes of tfidf-w2v features on test data :  (1608, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkk2r9PWKBn2",
        "outputId": "33b89f7a-2fd1-4f13-88e8-64e4c6bbbe5b"
      },
      "source": [
        "X_train_glove_tfidf_w2v=tfidf_w2v(X_train,glove_word_embeddings,glove_words)\r\n",
        "X_cv_glove_tfidf_w2v=tfidf_w2v(X_cv,glove_word_embeddings,glove_words)\r\n",
        "X_test_glove_tfidf_w2v=tfidf_w2v(X_test,glove_word_embeddings,glove_words)\r\n",
        "X_train_glove_tfidf_w2v=pd.DataFrame(X_train_glove_tfidf_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "X_cv_glove_tfidf_w2v=pd.DataFrame(X_cv_glove_tfidf_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "X_test_glove_tfidf_w2v=pd.DataFrame(X_test_glove_tfidf_w2v,columns=['embed_'+str(i) for i in range(300)])\r\n",
        "print('\\nAfter glove-tfidf_w2v \\n')\r\n",
        "print('Shapes of tfidf-w2v features on train data : ',X_train_glove_tfidf_w2v.shape)\r\n",
        "print('Shapes of tfidf-w2v features on cv data : ',X_cv_glove_tfidf_w2v.shape)\r\n",
        "print('Shapes of tfidf-w2v features on test data : ',X_test_glove_tfidf_w2v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "After glove-tfidf_w2v \n",
            "\n",
            "Shapes of tfidf-w2v features on train data :  (6659, 300)\n",
            "Shapes of tfidf-w2v features on cv data :  (940, 300)\n",
            "Shapes of tfidf-w2v features on test data :  (1608, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmJparwS4Tu2"
      },
      "source": [
        "def get_word_char_lengths(data):\r\n",
        "  '''This function takes input dataframe and return with length of text by wordlevel and characterleve'''\r\n",
        "  length_features=[]\r\n",
        "  for index,row in data.iterrows():\r\n",
        "    text=row['Description']\r\n",
        "    length_wordlevel=len(text.split()) # Getting the number of words\r\n",
        "    len_charlevel=len(text) # Getting the number characters including spaces\r\n",
        "    length_features.append([length_wordlevel,len_charlevel])\r\n",
        "  return pd.DataFrame(length_features,columns=['length_word_level','length_char_level'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXk5HOrw5tj9",
        "outputId": "c4bf328d-8f52-447d-d6ce-a6893b286daa"
      },
      "source": [
        "X_train_length_features=get_word_char_lengths(X_train)\r\n",
        "X_cv_length_features=get_word_char_lengths(X_cv)\r\n",
        "X_test_length_features=get_word_char_lengths(X_test)\r\n",
        "print('\\nAfter extracting length based features\\n')\r\n",
        "print('Shapes of length based features  on train data : ',X_train_length_features.shape )\r\n",
        "print('Shapes of length based features  on cv data : ',X_cv_length_features.shape)\r\n",
        "print('Shapes of length based features  on test data : ',X_test_length_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "After extracting length based features\n",
            "\n",
            "Shapes of length based features  on train data :  (6659, 2)\n",
            "Shapes of length based features  on cv data :  (940, 2)\n",
            "Shapes of length based features  on test data :  (1608, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SDYnlahEXU2",
        "outputId": "3fe09ca5-c21b-44bc-a07a-d89d72835a33"
      },
      "source": [
        "nltk.download('vader_lexicon')\r\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gyil2w3EY_z"
      },
      "source": [
        "def sentiment_score(data):\r\n",
        "  '''This function takes dataframe as input and returns sentiment scores of text data'''\r\n",
        "  sentiments=[]\r\n",
        "  preprocessed_descriptions = data['Description'].values\r\n",
        "  for text in preprocessed_descriptions:\r\n",
        "    polarities=sid.polarity_scores(text) # Getting the sentiment scores of text\r\n",
        "    sentiments.append(list(polarities.values()))\r\n",
        "  return pd.DataFrame(sentiments,columns=['negative','neutral','positive','compound'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwt9Jpw4Ep7g",
        "outputId": "9bbf2495-933c-43e7-9743-1e5b7a91d971"
      },
      "source": [
        "X_train_sentiment_scores=sentiment_score(X_train)\r\n",
        "X_cv_sentiment_scores=sentiment_score(X_cv)\r\n",
        "X_test_sentiment_scores=sentiment_score(X_test)\r\n",
        "print('\\nAfter extracting sentiment features\\n')\r\n",
        "print('Shapes of sentiment features on train data : ',X_train_sentiment_scores.shape)\r\n",
        "print('Shapes of sentiment features on cv data : ',X_cv_sentiment_scores.shape)\r\n",
        "print('Shapes of sentiment features on test data : ',X_test_sentiment_scores.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "After extracting sentiment features\n",
            "\n",
            "Shapes of sentiment features on train data :  (6659, 4)\n",
            "Shapes of sentiment features on cv data :  (940, 4)\n",
            "Shapes of sentiment features on test data :  (1608, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow1tB3N9UD9a"
      },
      "source": [
        "# Concatenating all the features that are extracted from the input\r\n",
        "#With fastext-w2v features\r\n",
        "X_train_final = pd.concat([X_train_fasttext_w2v,X_train_length_features,X_train_sentiment_scores],axis=1)\r\n",
        "X_cv_final = pd.concat([X_cv_fasttext_w2v,X_cv_length_features,X_cv_sentiment_scores],axis=1)\r\n",
        "X_test_final = pd.concat([X_test_fasttext_w2v,X_test_length_features,X_test_sentiment_scores],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcdC15u2VVba",
        "outputId": "32707446-4d84-401f-aa61-2c235f233b3f"
      },
      "source": [
        "print('Shape of final train data : ',X_train_final.shape,',  y_train shape : ', y_train.shape)\r\n",
        "print('Shape of final cv data : ',X_cv_final.shape, '   ,  y_cv shape : ',y_cv.shape)\r\n",
        "print('Shape of final test data : ',X_test_final.shape, ',  y_test shape : ',y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of final train data :  (6659, 306) ,  y_train shape :  (6659, 3)\n",
            "Shape of final cv data :  (940, 306)    ,  y_cv shape :  (940, 3)\n",
            "Shape of final test data :  (1608, 306) ,  y_test shape :  (1608, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCEXHOurKj0l"
      },
      "source": [
        "#with fasttext tfidf features\r\n",
        "X_train_final_1 = pd.concat([X_train_fasttext_tfidf_w2v,X_train_length_features,X_train_sentiment_scores],axis=1)\r\n",
        "X_cv_final_1 = pd.concat([X_cv_fasttext_tfidf_w2v,X_cv_length_features,X_cv_sentiment_scores],axis=1)\r\n",
        "X_test_final_1 = pd.concat([X_test_fasttext_tfidf_w2v,X_test_length_features,X_test_sentiment_scores],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8GPLbg9Klu5"
      },
      "source": [
        "#with  glove features\r\n",
        "X_train_final_2 = pd.concat([X_train_glove_w2v,X_train_length_features,X_train_sentiment_scores],axis=1)\r\n",
        "X_cv_final_2 = pd.concat([X_cv_glove_w2v,X_cv_length_features,X_cv_sentiment_scores],axis=1)\r\n",
        "X_test_final_2 = pd.concat([X_test_glove_w2v,X_test_length_features,X_test_sentiment_scores],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8PoktKbKmRE"
      },
      "source": [
        "#with glove tfidf features\r\n",
        "X_train_final_3 = pd.concat([X_train_glove_tfidf_w2v,X_train_length_features,X_train_sentiment_scores],axis=1)\r\n",
        "X_cv_final_3 = pd.concat([X_cv_glove_tfidf_w2v,X_cv_length_features,X_cv_sentiment_scores],axis=1)\r\n",
        "X_test_final_3 = pd.concat([X_test_glove_tfidf_w2v,X_test_length_features,X_test_sentiment_scores],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldiNqjREMRy7"
      },
      "source": [
        "After feature engineering we get vector representation of text which is 306-dimensions."
      ]
    }
  ]
}